{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *  NLP 101 *\n",
        "This notebook will be covering both classical NLP and deep learning NLP techniques.\n",
        "Let's import some packages that we will be using first. We will be first using nltk - the natural language toolkit to show us some classical NLP capabilities. These are important as we can still use these techniques in conjunction with our ML/DL models as preprocessing steps."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-11-10T19:43:31.776574Z",
          "iopub.execute_input": "2022-11-10T19:43:31.777568Z",
          "iopub.status.idle": "2022-11-10T19:43:31.784167Z",
          "shell.execute_reply.started": "2022-11-10T19:43:31.777520Z",
          "shell.execute_reply": "2022-11-10T19:43:31.782765Z"
        },
        "id": "jGaANr4Uupz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:13.615762Z",
          "iopub.execute_input": "2022-11-11T12:47:13.616446Z",
          "iopub.status.idle": "2022-11-11T12:47:15.278745Z",
          "shell.execute_reply.started": "2022-11-11T12:47:13.616392Z",
          "shell.execute_reply": "2022-11-11T12:47:15.277436Z"
        },
        "trusted": true,
        "id": "jjMRKC9Nup0C",
        "outputId": "e1692229-d295-4a19-d57b-e713a33291a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
          "output_type": "stream"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Classical NLP\n",
        "Let's define an input paragraph that we want to analyze first"
      ],
      "metadata": {
        "id": "-cru8C2Sup0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change the text - this is from Wikipedia\n",
        "text = \"Tower Bridge is a drawbridge in London. It crosses the River Thames near the Tower of London. It allows ships through the bridge deck when is raised at an angle in the centre. The north side of the bridge is Tower Hill, and the south side of the bridge comes down into Bermondsey, an area in Southwark. Tower Bridge is far more visible than London Bridge, which people often mistake it for. Many tourists go to London to see the Tower Bridge. It has its own exhibition centre in the horizontal walkway. This gives one of the best vantage points in London.\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.280946Z",
          "iopub.execute_input": "2022-11-11T12:47:15.282187Z",
          "iopub.status.idle": "2022-11-11T12:47:15.288935Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.282134Z",
          "shell.execute_reply": "2022-11-11T12:47:15.286973Z"
        },
        "trusted": true,
        "id": "SZ1Z90klup0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Sentence segmentation\n",
        "Split the sentences up"
      ],
      "metadata": {
        "id": "nmqSW4G6up0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.290966Z",
          "iopub.execute_input": "2022-11-11T12:47:15.291856Z",
          "iopub.status.idle": "2022-11-11T12:47:15.320763Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.291806Z",
          "shell.execute_reply": "2022-11-11T12:47:15.319632Z"
        },
        "trusted": true,
        "id": "MhBi1FaUup0G",
        "outputId": "8a43c56b-1f1b-4728-f778-f573a8676f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['Tower Bridge is a drawbridge in London.', 'It crosses the River Thames near the Tower of London.', 'It allows ships through the bridge deck when is raised at an angle in the centre.', 'The north side of the bridge is Tower Hill, and the south side of the bridge comes down into Bermondsey, an area in Southwark.', 'Tower Bridge is far more visible than London Bridge, which people often mistake it for.', 'Many tourists go to London to see the Tower Bridge.', 'It has its own exhibition centre in the horizontal walkway.', 'This gives one of the best vantage points in London.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenization\n",
        "Split them into individual words! If this is too easy go look up tokenization in Japanese which cannot rely on spaces to split the words. -> Search 'MeCab Japanese'"
      ],
      "metadata": {
        "id": "7zDVLOhPup0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.323615Z",
          "iopub.execute_input": "2022-11-11T12:47:15.324130Z",
          "iopub.status.idle": "2022-11-11T12:47:15.332034Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.324092Z",
          "shell.execute_reply": "2022-11-11T12:47:15.330570Z"
        },
        "trusted": true,
        "id": "l0h9Xalvup0H",
        "outputId": "f8c548c5-eb89-4e27-9ea5-ed239e51a20b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['Tower', 'Bridge', 'is', 'a', 'drawbridge', 'in', 'London', '.', 'It', 'crosses', 'the', 'River', 'Thames', 'near', 'the', 'Tower', 'of', 'London', '.', 'It', 'allows', 'ships', 'through', 'the', 'bridge', 'deck', 'when', 'is', 'raised', 'at', 'an', 'angle', 'in', 'the', 'centre', '.', 'The', 'north', 'side', 'of', 'the', 'bridge', 'is', 'Tower', 'Hill', ',', 'and', 'the', 'south', 'side', 'of', 'the', 'bridge', 'comes', 'down', 'into', 'Bermondsey', ',', 'an', 'area', 'in', 'Southwark', '.', 'Tower', 'Bridge', 'is', 'far', 'more', 'visible', 'than', 'London', 'Bridge', ',', 'which', 'people', 'often', 'mistake', 'it', 'for', '.', 'Many', 'tourists', 'go', 'to', 'London', 'to', 'see', 'the', 'Tower', 'Bridge', '.', 'It', 'has', 'its', 'own', 'exhibition', 'centre', 'in', 'the', 'horizontal', 'walkway', '.', 'This', 'gives', 'one', 'of', 'the', 'best', 'vantage', 'points', 'in', 'London', '.']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seems like we haven't kept the work we did in the sentence segmentation section - how should we apply word tokenization on top of sentence segmentation? Analysing sentences one at a time is usually preferable to minimize complexity."
      ],
      "metadata": {
        "id": "NGLVV-06up0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here!\n",
        "words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "print(words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.333428Z",
          "iopub.execute_input": "2022-11-11T12:47:15.333810Z",
          "iopub.status.idle": "2022-11-11T12:47:15.345722Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.333777Z",
          "shell.execute_reply": "2022-11-11T12:47:15.344387Z"
        },
        "trusted": true,
        "id": "XEaM-IlKup0J",
        "outputId": "01132c53-86c5-4de2-f4dd-9d983f3047ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[['Tower', 'Bridge', 'is', 'a', 'drawbridge', 'in', 'London', '.'], ['It', 'crosses', 'the', 'River', 'Thames', 'near', 'the', 'Tower', 'of', 'London', '.'], ['It', 'allows', 'ships', 'through', 'the', 'bridge', 'deck', 'when', 'is', 'raised', 'at', 'an', 'angle', 'in', 'the', 'centre', '.'], ['The', 'north', 'side', 'of', 'the', 'bridge', 'is', 'Tower', 'Hill', ',', 'and', 'the', 'south', 'side', 'of', 'the', 'bridge', 'comes', 'down', 'into', 'Bermondsey', ',', 'an', 'area', 'in', 'Southwark', '.'], ['Tower', 'Bridge', 'is', 'far', 'more', 'visible', 'than', 'London', 'Bridge', ',', 'which', 'people', 'often', 'mistake', 'it', 'for', '.'], ['Many', 'tourists', 'go', 'to', 'London', 'to', 'see', 'the', 'Tower', 'Bridge', '.'], ['It', 'has', 'its', 'own', 'exhibition', 'centre', 'in', 'the', 'horizontal', 'walkway', '.'], ['This', 'gives', 'one', 'of', 'the', 'best', 'vantage', 'points', 'in', 'London', '.']]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Part-of-speech tagging\n",
        "Now we want to analyse the first sentence to see what parts of speech are present. Usually we can use just the nouns present to guess what is going on in a sentence."
      ],
      "metadata": {
        "id": "0nSfg7d3up0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(sentences[0])\n",
        "print(f'First sentence tokenized: {words}')\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "print(f'Part of speech tags: {pos_tags}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.347375Z",
          "iopub.execute_input": "2022-11-11T12:47:15.347788Z",
          "iopub.status.idle": "2022-11-11T12:47:15.503168Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.347755Z",
          "shell.execute_reply": "2022-11-11T12:47:15.501830Z"
        },
        "trusted": true,
        "id": "jvJnDw2cup0J",
        "outputId": "4030bc98-f16a-445b-9814-67affc835c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "First sentence tokenized: ['Tower', 'Bridge', 'is', 'a', 'drawbridge', 'in', 'London', '.']\nPart of speech tags: [('Tower', 'NNP'), ('Bridge', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('drawbridge', 'NN'), ('in', 'IN'), ('London', 'NNP'), ('.', '.')]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Normalization\n",
        "We want to reduce vocabulary size which will help improve our NLP.\n",
        "#### 4.1 Stemming\n",
        "Simply truncating words to their stems (may not be words themselves). Let's try 2 different stemmers over some examples. Snowball stemmer is the 'upgraded' version of the Porter stemmer, including the ability for users to not process stopwords as sometimes the conjugated forms may have different meanings, e.g. 'to be' and 'a being'."
      ],
      "metadata": {
        "id": "8znoJjiWup0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your own word list\n",
        "word_list = ['being', 'a', 'fairly', 'mischievous', 'cat', 'causing', 'trouble', 'late', 'into', 'the', 'night']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.506814Z",
          "iopub.execute_input": "2022-11-11T12:47:15.507226Z",
          "iopub.status.idle": "2022-11-11T12:47:15.514467Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.507190Z",
          "shell.execute_reply": "2022-11-11T12:47:15.512986Z"
        },
        "trusted": true,
        "id": "RqqcKrg3up0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = nltk.stem.PorterStemmer()\n",
        "snowball = nltk.stem.SnowballStemmer('english', ignore_stopwords = True)\n",
        "\n",
        "print([porter.stem(word) for word in word_list])\n",
        "print([snowball.stem(word) for word in word_list])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.516571Z",
          "iopub.execute_input": "2022-11-11T12:47:15.517190Z",
          "iopub.status.idle": "2022-11-11T12:47:15.536395Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.517113Z",
          "shell.execute_reply": "2022-11-11T12:47:15.534733Z"
        },
        "trusted": true,
        "id": "ASd5ORw5up0K",
        "outputId": "b5882a76-853d-4cad-b539-688b89b49f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['be', 'a', 'fairli', 'mischiev', 'cat', 'caus', 'troubl', 'late', 'into', 'the', 'night']\n['being', 'a', 'fair', 'mischiev', 'cat', 'caus', 'troubl', 'late', 'into', 'the', 'night']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2 Lemmatisation\n",
        "Another option is to reduce words to their base lemmas to 'standardize' words into their synonyms. Lemmatisation is better usually as it is more informative and uses PoS. Let's run the same sentence we stemmed in our lemmatizer."
      ],
      "metadata": {
        "id": "kytke53Nup0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert our PoS tags from one type to another used by wordnet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return nltk.corpus.wordnet.ADV\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.538860Z",
          "iopub.execute_input": "2022-11-11T12:47:15.540385Z",
          "iopub.status.idle": "2022-11-11T12:47:15.549339Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.540328Z",
          "shell.execute_reply": "2022-11-11T12:47:15.547719Z"
        },
        "trusted": true,
        "id": "5LQEd4T7up0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "pos_tags = nltk.pos_tag(word_list) # we need the part of speech tags for lemmatisation\n",
        "print(pos_tags)\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(pos_tag[0], get_wordnet_pos(pos_tag[1])) if get_wordnet_pos(pos_tag[1]) else pos_tag[0] for pos_tag in pos_tags]\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:15.555557Z",
          "iopub.execute_input": "2022-11-11T12:47:15.555980Z",
          "iopub.status.idle": "2022-11-11T12:47:17.634876Z",
          "shell.execute_reply.started": "2022-11-11T12:47:15.555945Z",
          "shell.execute_reply": "2022-11-11T12:47:17.633618Z"
        },
        "trusted": true,
        "id": "T5wV7ToUup0L",
        "outputId": "11971fa5-b12d-4e09-a739-432c59ea9eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[('being', 'VBG'), ('a', 'DT'), ('fairly', 'RB'), ('mischievous', 'JJ'), ('cat', 'NN'), ('causing', 'VBG'), ('trouble', 'NN'), ('late', 'RB'), ('into', 'IN'), ('the', 'DT'), ('night', 'NN')]\n['be', 'a', 'fairly', 'mischievous', 'cat', 'cause', 'trouble', 'late', 'into', 'the', 'night']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Removing stopwords\n",
        "We remove stopwords to get rid of noise that is usually irrelevant."
      ],
      "metadata": {
        "id": "zQVIUi1uup0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "without_stop_words = [word for word in lemmatized_words if not word in stop_words]\n",
        "print(without_stop_words)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:17.636596Z",
          "iopub.execute_input": "2022-11-11T12:47:17.637367Z",
          "iopub.status.idle": "2022-11-11T12:47:17.645372Z",
          "shell.execute_reply.started": "2022-11-11T12:47:17.637320Z",
          "shell.execute_reply": "2022-11-11T12:47:17.644025Z"
        },
        "trusted": true,
        "id": "2fGwTHy3up0M",
        "outputId": "3f1e4d56-536a-4cb9-db4d-c1b25ee8d883"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['fairly', 'mischievous', 'cat', 'cause', 'trouble', 'late', 'night']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Regex and Fuzzy Matching\n",
        "Let's import some packages we will be using!"
      ],
      "metadata": {
        "id": "y-bmjGLhup0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from fuzzywuzzy import fuzz\n",
        "! pip install pyspellchecker\n",
        "from spellchecker import SpellChecker"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-14T09:35:14.695340Z",
          "iopub.execute_input": "2023-07-14T09:35:14.696121Z",
          "iopub.status.idle": "2023-07-14T09:35:14.816213Z",
          "shell.execute_reply.started": "2023-07-14T09:35:14.696000Z",
          "shell.execute_reply": "2023-07-14T09:35:14.815332Z"
        },
        "trusted": true,
        "id": "mMa9FkLeup0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Regex\n",
        "Let's try regular expression matching"
      ],
      "metadata": {
        "id": "aZH258c3up0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.sub('a', 'b', 'abacadabra is a magic spell'))\n",
        "print(re.findall(r'[A-Z]+', 'I am feeling okay but if I am happy I ONLY USE CAPS'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.655760Z",
          "iopub.execute_input": "2022-11-11T12:47:32.656316Z",
          "iopub.status.idle": "2022-11-11T12:47:32.663518Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.656250Z",
          "shell.execute_reply": "2022-11-11T12:47:32.662351Z"
        },
        "trusted": true,
        "id": "aKwGZTZgup0O",
        "outputId": "2e44982c-2e01-4a1c-cff2-ce9b6cd57a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "bbbcbdbbrb is b mbgic spell\n['I', 'I', 'I', 'ONLY', 'USE', 'CAPS']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, your turn. Please Google to look up new expressions! regex101.com is great for trying out expressions too\n",
        "\n",
        "Task 1: fill in the regex expression to match only on ['1YESa', 'aYES3', 'asdkfYES', 'YES']."
      ],
      "metadata": {
        "id": "pR1bKtJhup0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_1 = '1YESa 2yesb aYES3 asdkfYES YES yES6'\n",
        "print(re.findall(r'[a-z0-9]*YES[a-z0-9]*', task_1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-14T09:35:22.037041Z",
          "iopub.execute_input": "2023-07-14T09:35:22.037461Z",
          "iopub.status.idle": "2023-07-14T09:35:22.043810Z",
          "shell.execute_reply.started": "2023-07-14T09:35:22.037429Z",
          "shell.execute_reply": "2023-07-14T09:35:22.042847Z"
        },
        "trusted": true,
        "id": "vCpFyU7dup0P",
        "outputId": "b6250165-72e5-4cbc-9098-efb6f40ebb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['1YESa', 'aYES3', 'asdkfYES', 'YES']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_1 = '1YESa 2yesb aYES3 asdkfYES YES yES6'\n",
        "print(re.findall(r'[a-z]+YES[0-9]*', task_1))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-14T09:43:45.100231Z",
          "iopub.execute_input": "2023-07-14T09:43:45.100714Z",
          "iopub.status.idle": "2023-07-14T09:43:45.107131Z",
          "shell.execute_reply.started": "2023-07-14T09:43:45.100676Z",
          "shell.execute_reply": "2023-07-14T09:43:45.105851Z"
        },
        "trusted": true,
        "id": "kZS4_pXBup0P",
        "outputId": "25340f10-89b5-45e8-b925-cb770f6d7246"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['aYES3', 'asdkfYES']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: fill in the regex expression to match all 6 e-mail addresses in the string"
      ],
      "metadata": {
        "id": "Phfbkfvtup0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task_2 = 'Hi Sally, Could you please forward this e-mail to the e-mails in this list please? l&d@deloitte.co.uk, nlp101@deloitte.co.uk and aitesting@deloitte.com, ai101@deloitte.com and ukai@deloitte.com, l&d@deloitte.com Thanks! My instagram handle is @swiftnlp.'\n",
        "print(task_2, '\\n')\n",
        "print(re.findall(r'YOUR REGEX STRING HERE', task_2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.694148Z",
          "iopub.execute_input": "2022-11-11T12:47:32.694694Z",
          "iopub.status.idle": "2022-11-11T12:47:32.708635Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.694642Z",
          "shell.execute_reply": "2022-11-11T12:47:32.707621Z"
        },
        "trusted": true,
        "id": "L0NA-WT0up0Q",
        "outputId": "934b2cbf-39d5-4e57-bc42-616722758fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Hi Sally, Could you please forward this e-mail to the e-mails in this list please? l&d@deloitte.co.uk, nlp101@deloitte.co.uk and aitesting@deloitte.com, ai101@deloitte.com and ukai@deloitte.com, l&d@deloitte.com Thanks! My instagram handle is @swiftnlp. \n\n[]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_2 = 'Hi Sally, Could you please forward this e-mail to the e-mails in this list please? l&d@deloitte.co.uk, nlp101@deloitte.co.uk and aitesting@deloitte.com, ai101@deloitte.com and ukai@deloitte.com, l&d@deloitte.com Thanks! My instagram handle is @swiftnlp.'\n",
        "print(task_2, '\\n')\n",
        "print(re.findall(r'[A-Za-z0-9&]+\\@[A-Za-z]+(?:\\.[A-Za-z]+)+', task_2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.710622Z",
          "iopub.execute_input": "2022-11-11T12:47:32.710978Z",
          "iopub.status.idle": "2022-11-11T12:47:32.721914Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.710947Z",
          "shell.execute_reply": "2022-11-11T12:47:32.720669Z"
        },
        "trusted": true,
        "id": "aLldYaCHup0Q",
        "outputId": "b7e8467f-251e-4278-c722-b1eb66bf349a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Hi Sally, Could you please forward this e-mail to the e-mails in this list please? l&d@deloitte.co.uk, nlp101@deloitte.co.uk and aitesting@deloitte.com, ai101@deloitte.com and ukai@deloitte.com, l&d@deloitte.com Thanks! My instagram handle is @swiftnlp. \n\n['l&d@deloitte.co.uk', 'nlp101@deloitte.co.uk', 'aitesting@deloitte.com', 'ai101@deloitte.com', 'ukai@deloitte.com', 'l&d@deloitte.com']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Fuzzy Matching\n",
        "\n",
        "Quick method for matching strings that are similar but not exactly the same - great for data cleaning!"
      ],
      "metadata": {
        "id": "H18wdIqvup0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string1 = 'Online NLP 101 Course for Everyone!'\n",
        "string2 = 'Course in NLP'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.723914Z",
          "iopub.execute_input": "2022-11-11T12:47:32.724386Z",
          "iopub.status.idle": "2022-11-11T12:47:32.733716Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.724338Z",
          "shell.execute_reply": "2022-11-11T12:47:32.732449Z"
        },
        "trusted": true,
        "id": "MuNvmQVuup0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fuzz.ratio(string1, string2))\n",
        "print(fuzz.partial_ratio(string1, string2))\n",
        "print(fuzz.token_sort_ratio(string1, string2))\n",
        "print(fuzz.token_set_ratio(string1, string2))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.735549Z",
          "iopub.execute_input": "2022-11-11T12:47:32.735956Z",
          "iopub.status.idle": "2022-11-11T12:47:32.753286Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.735915Z",
          "shell.execute_reply": "2022-11-11T12:47:32.751757Z"
        },
        "trusted": true,
        "id": "6FPtcKnCup0R",
        "outputId": "226206db-7f7c-437b-81f5-73166ca92028"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "33\n62\n51\n87\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another use case for fuzzy matching is simple spell checking using pyspellchecker which relies on Levenshtein distance but also the frequency the word appears in the English language. You can then replace these spelling errors!"
      ],
      "metadata": {
        "id": "SMYDbcsgup0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
        "\n",
        "for word in misspelled:\n",
        "    # Get the one `most likely` answer\n",
        "    print('most likely candidate: ', spell.correction(word))\n",
        "\n",
        "    # Get a list of `likely` options\n",
        "    print('all likely candidates: ', spell.candidates(word))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:32.754930Z",
          "iopub.execute_input": "2022-11-11T12:47:32.755369Z",
          "iopub.status.idle": "2022-11-11T12:47:33.533014Z",
          "shell.execute_reply.started": "2022-11-11T12:47:32.755334Z",
          "shell.execute_reply": "2022-11-11T12:47:33.531561Z"
        },
        "trusted": true,
        "id": "Nzx0VZYHup0S",
        "outputId": "84adf04f-e164-4a62-8a12-aaa01cd8a1a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "most likely candidate:  happening\nall likely candidates:  {'happening', 'henning', 'penning'}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. ML & DL NLP\n",
        "### 1. Named Entity Recognition\n",
        "Let us use a popular NLP package named Spacy which will also do all the pre-processing/Classic NLP steps above!"
      ],
      "metadata": {
        "id": "WGYPw1Yyup0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "! pip install contextualSpellCheck\n",
        "import contextualSpellCheck"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:47:33.534426Z",
          "iopub.execute_input": "2022-11-11T12:47:33.534859Z",
          "iopub.status.idle": "2022-11-11T12:48:13.231012Z",
          "shell.execute_reply.started": "2022-11-11T12:47:33.534822Z",
          "shell.execute_reply": "2022-11-11T12:48:13.229609Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "5I4x4Z5Xup0S",
        "outputId": "a71b0ed0-1b39-472d-9cca-ea1e9371c414"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting contextualSpellCheck\n  Downloading contextualSpellCheck-0.4.3-py3-none-any.whl (128 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m635.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting editdistance==0.6.0\n  Downloading editdistance-0.6.0-cp37-cp37m-manylinux2010_x86_64.whl (285 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.6/285.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from contextualSpellCheck) (3.3.1)\nRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from contextualSpellCheck) (1.11.0+cpu)\nRequirement already satisfied: transformers>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from contextualSpellCheck) (4.20.1)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.7)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (8.0.17)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (0.4.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (0.10.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (2.0.8)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (1.0.9)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (1.21.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (1.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (3.3.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (1.8.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (4.64.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.8)\nCollecting typing-extensions<4.2.0,>=3.7.4\n  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (3.1.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (21.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (2.28.1)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (2.4.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (3.0.10)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (0.7.9)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (0.6.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy>=3.0.0->contextualSpellCheck) (59.8.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (2021.11.10)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (0.10.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (0.12.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (4.13.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (6.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.0.0->contextualSpellCheck) (3.7.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0.0->contextualSpellCheck) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy>=3.0.0->contextualSpellCheck) (3.0.9)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy>=3.0.0->contextualSpellCheck) (5.2.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.0->contextualSpellCheck) (2.1.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->contextualSpellCheck) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy>=3.0.0->contextualSpellCheck) (2.1.1)\nInstalling collected packages: typing-extensions, editdistance, contextualSpellCheck\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Uninstalling typing_extensions-4.4.0:\n      Successfully uninstalled typing_extensions-4.4.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires pyarrow<6,>=1, but you have pyarrow 8.0.0 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\napache-beam 2.40.0 requires pyarrow<8.0.0,>=0.15.1, but you have pyarrow 8.0.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed contextualSpellCheck-0.4.3 editdistance-0.6.0 typing-extensions-4.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:13.232880Z",
          "iopub.execute_input": "2022-11-11T12:48:13.234073Z",
          "iopub.status.idle": "2022-11-11T12:48:14.155228Z",
          "shell.execute_reply.started": "2022-11-11T12:48:13.234009Z",
          "shell.execute_reply": "2022-11-11T12:48:14.153817Z"
        },
        "trusted": true,
        "id": "X5HdBV5Cup0T",
        "outputId": "223c566f-93a7-4d05-84c4-7ef4e5c54db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tower Bridge\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n</mark>\n is a drawbridge in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    London\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n. It crosses the River Thames near \n<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    the Tower of London\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n</mark>\n. It allows ships through the bridge deck when is raised at an angle in the centre. The north side of the bridge is \n<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tower Hill\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n</mark>\n, and the south side of the bridge comes down into \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Bermondsey\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n, an area in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Southwark\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n. \n<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tower Bridge\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n</mark>\n is far more visible than \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    London Bridge\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n, which people often mistake it for. Many tourists go to \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    London\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n to see \n<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    the Tower Bridge\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n</mark>\n. It has its own exhibition centre in the horizontal walkway. This gives \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    one\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n of the best vantage points in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    London\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n.</div></span>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. BERT use-case: Contextual spell checking\n",
        "Let's use spacy for contextual spell checking too!"
      ],
      "metadata": {
        "id": "K_HHXfCIup0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contextualSpellCheck.add_to_pipe(nlp)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:14.156767Z",
          "iopub.execute_input": "2022-11-11T12:48:14.157123Z",
          "iopub.status.idle": "2022-11-11T12:48:34.356011Z",
          "shell.execute_reply.started": "2022-11-11T12:48:14.157090Z",
          "shell.execute_reply": "2022-11-11T12:48:34.354252Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "59bc83ee21d14f8da9f27a8454892c0b",
            "30f5baee98b044be839bbedf215a43a6",
            "ba51c5db232f4b729ba700f45834079d",
            "75488df558b44b23b267044293a1c865",
            "3fb776ac15644fa2871e0a1c0e0fcd87"
          ]
        },
        "id": "i_G1jegNup0T",
        "outputId": "4146973f-a82d-4afa-f67d-08e00fc0ed39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59bc83ee21d14f8da9f27a8454892c0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30f5baee98b044be839bbedf215a43a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba51c5db232f4b729ba700f45834079d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75488df558b44b23b267044293a1c865"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fb776ac15644fa2871e0a1c0e0fcd87"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('Incom was $9.4 milion conpar to the prior year of $2.7 milion.')\n",
        "doc._.outcome_spellCheck"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:34.359276Z",
          "iopub.execute_input": "2022-11-11T12:48:34.360268Z",
          "iopub.status.idle": "2022-11-11T12:48:36.689797Z",
          "shell.execute_reply.started": "2022-11-11T12:48:34.360215Z",
          "shell.execute_reply": "2022-11-11T12:48:36.688446Z"
        },
        "trusted": true,
        "id": "x4pz4mc7up0U",
        "outputId": "dd0e90b0-89ca-43cf-bbc7-5e48cfb44228"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'It was $9.4 million compared to the prior year of $2.7 million.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Sentiment Analysis\n",
        "\n",
        "Let us use NLTK's built-in, pretrained sentiment analyser. It is called VADER. VADER is best suited for social media language - short w/ abbreviations and slang."
      ],
      "metadata": {
        "id": "9u3R28G8up0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:36.691810Z",
          "iopub.execute_input": "2022-11-11T12:48:36.693411Z",
          "iopub.status.idle": "2022-11-11T12:48:36.728092Z",
          "shell.execute_reply.started": "2022-11-11T12:48:36.693358Z",
          "shell.execute_reply": "2022-11-11T12:48:36.726634Z"
        },
        "trusted": true,
        "id": "jNN0Y9LJup0V",
        "outputId": "23f08c4b-bc85-47b2-b306-46785ed59b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Build your own ML model for classification"
      ],
      "metadata": {
        "id": "OrfrMBxJup0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all required packages"
      ],
      "metadata": {
        "id": "Q631lRFAup0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "import contractions\n",
        "import string\n",
        "from itertools import chain\n",
        "from nltk.corpus import movie_reviews as mr\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import naive_bayes, linear_model, ensemble, svm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:36.729871Z",
          "iopub.execute_input": "2022-11-11T12:48:36.730268Z",
          "iopub.status.idle": "2022-11-11T12:48:52.256335Z",
          "shell.execute_reply.started": "2022-11-11T12:48:36.730232Z",
          "shell.execute_reply": "2022-11-11T12:48:52.254664Z"
        },
        "trusted": true,
        "id": "YJfoR-Qgup0W",
        "outputId": "751003f8-8c9e-4159-e307-10a7f69a7a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting contractions\n  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\nCollecting textsearch>=0.0.21\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nCollecting anyascii\n  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyahocorasick\n  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in data"
      ],
      "metadata": {
        "id": "W8oulH7tup0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
        "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
        "pos_revs = pd.DataFrame([nltk.corpus.movie_reviews.raw(rev) for rev in positive_review_ids], columns = ['review'])\n",
        "neg_revs = pd.DataFrame([nltk.corpus.movie_reviews.raw(rev) for rev in negative_review_ids], columns = ['review'])\n",
        "pos_revs['positive_sentiment'] = 1\n",
        "neg_revs['positive_sentiment'] = 0\n",
        "all_revs = pos_revs.append(neg_revs)\n",
        "all_revs.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:52.258900Z",
          "iopub.execute_input": "2022-11-11T12:48:52.259446Z",
          "iopub.status.idle": "2022-11-11T12:48:54.097793Z",
          "shell.execute_reply.started": "2022-11-11T12:48:52.259386Z",
          "shell.execute_reply": "2022-11-11T12:48:54.096487Z"
        },
        "trusted": true,
        "id": "zqwDvrzKup0X",
        "outputId": "b18d786a-0c31-49cb-d0ad-007e9a408090"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              review  positive_sentiment\n0  films adapted from comic books have had plenty...                   1\n1  every now and then a movie comes along from a ...                   1\n2  you've got mail works alot better than it dese...                   1\n3   \" jaws \" is a rare film that grabs your atten...                   1\n4  moviemaking is a lot like being the general ma...                   1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>positive_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>films adapted from comic books have had plenty...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>every now and then a movie comes along from a ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>you've got mail works alot better than it dese...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\" jaws \" is a rare film that grabs your atten...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>moviemaking is a lot like being the general ma...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply cleaning steps here\n",
        "1. Sentence split up\n",
        "2. Words split up\n",
        "3. Lemmatisation\n",
        "4. Taking stopwords out"
      ],
      "metadata": {
        "id": "P1RC_nlcup0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(word_list):\n",
        "    pos_tags = nltk.pos_tag(word_list) # we need the part of speech tags for lemmatisation\n",
        "\n",
        "    lemmatized_words = [lemmatizer.lemmatize(pos_tag[0], get_wordnet_pos(pos_tag[1])) if get_wordnet_pos(pos_tag[1]) else pos_tag[0] for pos_tag in pos_tags]\n",
        "    return lemmatized_words\n",
        "\n",
        "all_revs['review'] = all_revs['review'].apply(lambda x:' '.join([contractions.fix(word) for word in x.split()]))\n",
        "all_revs['review'] = all_revs['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "all_revs['review'] = all_revs['review'].apply(lambda x: ' '.join(lemmatize(nltk.word_tokenize(x))))\n",
        "\n",
        "all_revs.head(5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:48:54.104476Z",
          "iopub.execute_input": "2022-11-11T12:48:54.105251Z",
          "iopub.status.idle": "2022-11-11T12:50:12.740978Z",
          "shell.execute_reply.started": "2022-11-11T12:48:54.105210Z",
          "shell.execute_reply": "2022-11-11T12:50:12.739532Z"
        },
        "trusted": true,
        "id": "ACTJN-OFup0Y",
        "outputId": "8678fea6-1d44-405e-a95d-291c2fa7f010"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 29,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                              review  positive_sentiment\n0  film adapt comic book plenty success , whether...                   1\n1  every movie come along suspect studio , every ...                   1\n2  get mail work alot good deserves . order make ...                   1\n3  `` jaw `` rare film grab attention show single...                   1\n4  moviemaking lot like general manager nfl team ...                   1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>positive_sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>film adapt comic book plenty success , whether...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>every movie come along suspect studio , every ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>get mail work alot good deserves . order make ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>`` jaw `` rare film grab attention show single...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>moviemaking lot like general manager nfl team ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test set for model training"
      ],
      "metadata": {
        "id": "sQFVqo7jup0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, valid_x, train_y, valid_y = sklearn.model_selection.train_test_split(all_revs['review'], all_revs['positive_sentiment'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:50:12.742993Z",
          "iopub.execute_input": "2022-11-11T12:50:12.743648Z",
          "iopub.status.idle": "2022-11-11T12:50:12.752309Z",
          "shell.execute_reply.started": "2022-11-11T12:50:12.743605Z",
          "shell.execute_reply": "2022-11-11T12:50:12.750635Z"
        },
        "trusted": true,
        "id": "PB5v8D7-up0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize using 2 methods: count vectorization and tfidf vectorization"
      ],
      "metadata": {
        "id": "ysEmDQyOup0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = sklearn.feature_extraction.text.CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(all_revs['review'])\n",
        "tfidf_vect = sklearn.feature_extraction.text.TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features = 500)\n",
        "tfidf_vect.fit(all_revs['review'])\n",
        "\n",
        "xtrain_count = count_vect.transform(train_x)\n",
        "xvalid_count = count_vect.transform(valid_x)\n",
        "\n",
        "xtrain_tfidf = tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf = tfidf_vect.transform(valid_x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:50:12.755602Z",
          "iopub.execute_input": "2022-11-11T12:50:12.756671Z",
          "iopub.status.idle": "2022-11-11T12:50:15.332865Z",
          "shell.execute_reply.started": "2022-11-11T12:50:12.756621Z",
          "shell.execute_reply": "2022-11-11T12:50:15.331370Z"
        },
        "trusted": true,
        "id": "1z60mzdyup0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a couple of classic classification models:\n",
        "1. Naive Bayes\n",
        "2. Linear Model (Logistic Regression)\n",
        "3. Support Vector Machines\n",
        "4. Random Forest"
      ],
      "metadata": {
        "id": "P1oV5w2Xup0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function created to make it easier to train various models/algorithms\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return sklearn.metrics.classification_report(valid_y, predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:50:15.334536Z",
          "iopub.execute_input": "2022-11-11T12:50:15.335569Z",
          "iopub.status.idle": "2022-11-11T12:50:15.341652Z",
          "shell.execute_reply.started": "2022-11-11T12:50:15.335493Z",
          "shell.execute_reply": "2022-11-11T12:50:15.340063Z"
        },
        "trusted": true,
        "id": "m4zCmGpgup0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_cv = train_model(naive_bayes.BernoulliNB(), xtrain_count.toarray(), train_y, xvalid_count.toarray())\n",
        "print(\"Naive Bayes + Count Vectors: \", nb_cv)\n",
        "\n",
        "nb_tfidf = train_model(naive_bayes.BernoulliNB(), xtrain_tfidf.toarray(), train_y, xvalid_tfidf.toarray())\n",
        "print(\"Naive Bayes + TFIDF Vectors: \", nb_tfidf)\n",
        "\n",
        "lr_cv = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"Logistic Regression + Count Vectors: \", lr_cv)\n",
        "\n",
        "lr_tfidf = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"Logistic Regression + TFIDF Vectors: \", lr_tfidf)\n",
        "\n",
        "svm_cv = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"Support Vector Machines + Count Vectors: \", svm_cv)\n",
        "\n",
        "svm_tfidf = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"Support Vector Machines + TFIDF Vectors: \", svm_tfidf)\n",
        "\n",
        "rf_cv = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"Random Forest + Count Vectors: \", rf_cv)\n",
        "\n",
        "rf_tfidf = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"Random Forest + TFIDF Vectors: \", rf_tfidf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-11-11T12:57:55.528667Z",
          "iopub.execute_input": "2022-11-11T12:57:55.529183Z",
          "iopub.status.idle": "2022-11-11T12:58:14.863663Z",
          "shell.execute_reply.started": "2022-11-11T12:57:55.529142Z",
          "shell.execute_reply": "2022-11-11T12:58:14.862312Z"
        },
        "trusted": true,
        "id": "03bTv_KUup0b",
        "outputId": "ae8819b9-2566-4549-84d4-9006b2a266f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Naive Bayes + Count Vectors:                precision    recall  f1-score   support\n\n           0       0.74      0.88      0.80       246\n           1       0.86      0.70      0.77       254\n\n    accuracy                           0.79       500\n   macro avg       0.80      0.79      0.79       500\nweighted avg       0.80      0.79      0.79       500\n\nNaive Bayes + TFIDF Vectors:                precision    recall  f1-score   support\n\n           0       0.71      0.78      0.74       246\n           1       0.76      0.69      0.73       254\n\n    accuracy                           0.73       500\n   macro avg       0.74      0.73      0.73       500\nweighted avg       0.74      0.73      0.73       500\n\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Logistic Regression + Count Vectors:                precision    recall  f1-score   support\n\n           0       0.82      0.85      0.83       246\n           1       0.84      0.81      0.83       254\n\n    accuracy                           0.83       500\n   macro avg       0.83      0.83      0.83       500\nweighted avg       0.83      0.83      0.83       500\n\nLogistic Regression + TFIDF Vectors:                precision    recall  f1-score   support\n\n           0       0.78      0.80      0.79       246\n           1       0.80      0.78      0.79       254\n\n    accuracy                           0.79       500\n   macro avg       0.79      0.79      0.79       500\nweighted avg       0.79      0.79      0.79       500\n\nSupport Vector Machines + Count Vectors:                precision    recall  f1-score   support\n\n           0       0.78      0.83      0.81       246\n           1       0.83      0.77      0.80       254\n\n    accuracy                           0.80       500\n   macro avg       0.80      0.80      0.80       500\nweighted avg       0.80      0.80      0.80       500\n\nSupport Vector Machines + TFIDF Vectors:                precision    recall  f1-score   support\n\n           0       0.79      0.81      0.80       246\n           1       0.81      0.79      0.80       254\n\n    accuracy                           0.80       500\n   macro avg       0.80      0.80      0.80       500\nweighted avg       0.80      0.80      0.80       500\n\nRandom Forest + Count Vectors:                precision    recall  f1-score   support\n\n           0       0.81      0.80      0.81       246\n           1       0.81      0.81      0.81       254\n\n    accuracy                           0.81       500\n   macro avg       0.81      0.81      0.81       500\nweighted avg       0.81      0.81      0.81       500\n\nRandom Forest + TFIDF Vectors:                precision    recall  f1-score   support\n\n           0       0.76      0.84      0.80       246\n           1       0.83      0.75      0.79       254\n\n    accuracy                           0.79       500\n   macro avg       0.79      0.79      0.79       500\nweighted avg       0.80      0.79      0.79       500\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The challenge is now to improve the sentiment predictor. There are many methods but the easiest one is by preprocessing the text.\n",
        "\n",
        "If you don't know where to start, try:\n",
        "1. Removing contractions\n",
        "2. Removing stopwords\n",
        "3. Lemmatising the text\n",
        "\n",
        "After your text is clean and processed, you can then start looking at adding additional features (e.g. word count), optimizing the vectorization methods and classification algorithms."
      ],
      "metadata": {
        "id": "q44k4DFHup0c"
      }
    }
  ]
}